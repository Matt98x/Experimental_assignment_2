<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Experimental laboratory assignment 2: Experimental robotic assignment 2</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Experimental laboratory assignment 2
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Experimental robotic assignment 2 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="http://htmlpreview.github.io/?https://github.com/Matt98x/Experimental_assignment1/blob/main/pet_package/html/index.html" title="Documentation">Documentation</a></p>
<h2>Introduction</h2>
<p>This assignment target is to build an ROS architecture to implement a robot, simulating a pet, that interact with a human and moves in a discrete 2D environment. The pet has three states representing behaviours, that are simulated as a finite state machine with 3 states:</p><ul>
<li>Play</li>
<li>Sleep</li>
<li>Normal</li>
</ul>
<p>These states determine the way the robot act inside the grid, whether moving randomly as in normal, going to targets determined by the position of a ball guided by the human or simply sleeping in the home position.</p>
<p>The robot will change between states randomly, eccept for play, which is received by the user, by imposing a positive height with respect to the ground.</p>
<h2>Software Architecture, State Machine and communications</h2>
<h3>Architecture</h3>
<p>The software architecture consists of two main areas: the world and the pet, programmatically represented by the two packages that compose this implementation. The first handle how the world is organized and works externally to the pet. This means that the commander, the Pet-logic(now simply interpretable as Logic) and the movement of the ball can ba all thought to be component of this macro-architecture. Going to the Pet, this can be considered as connected to the world over just two aspect:</p><ul>
<li>The Perception: that handles how the pet perceives the world and in particular the ball</li>
<li>The Logic: which is simply a remaining part from the first assignment, in which this node controlled both the command interpretation and the control of the robot state. Here we show the architecture image: </li>
</ul>
<p>&lt;img src="https://github.com/Matt98x/Experimental_assignment_2/blob/main/Images/Component_diagrams.PNG?raw=true "Title""&gt; </p>
<p>Component Diagram </p>
<p>Going in depth of the components, we have:</p><ul>
<li>Random command generator(<a class="el" href="Command__giver_8py.html" title="Component that gives command either randomly generated or given by a user, switching between the two...">Command_giver.py</a>): randomically send a string representing the concatenation of one or more commands of the form: 'play'(to start the play state),'point to x y'(to simulate the pointing gesture to x y),'go to x y'(to simulate the voice command to x y) and 'hide'(to stop the play state without entering the sleep state)</li>
<li>Interpreter(<a class="el" href="Pet__logic_8py.html" title="Pet logic. ">Pet_logic.py</a>): to interpret the string commands and translate them to movement of the ball, moreover, it handles the switch from play and normal to sleep and from sleep to normal</li>
<li>Pet behaviours(<a class="el" href="Pet__behaviours_8py.html" title="Pet state machine. ">Pet_behaviours.py</a>)- that simulate behaviours as a finite state, in the already mentioned states</li>
<li>Perception(<a class="el" href="robot__following_8py.html" title="Perception algorithm which handles the ball tracking and the swing routine. ">robot_following.py</a>): which is the node that handles the camera inputs(target identification and research) and the hardware control for these tasks(control of the neck joint(target tracking) and body(target search) )</li>
<li>Actor: may or may not be present and provides the same type of messages that the Command_giver provides, adding also symbolical location such as "home" and "owner", moreover can query or set the state of the robot and interact with the parameters.</li>
<li>Ball: representation of the logic controlling the ball</li>
<li>Robot Control: representation of the low-level control of the ball</li>
<li>Gazebo: although not a logic part of the component diagram, it represent the way ball, robot control and perception are fundamentally part of the simulation and communicate with it</li>
</ul>
<p>Here we can see how these elements communicate between themselves and with the Gazebo simulation environment: </p>
<p>&lt;img src="https://github.com/Matt98x/Experimental_assignment_2/blob/main/Images/rqt_graph.PNG?raw=true "Title""&gt; </p>
<p>Component Diagram </p>
<h3>State Machine</h3>
<p>Now, we can discuss the finite state machine. This, can be described by the following image:</p>
<p>&lt;img src="https://github.com/Matt98x/Experimental_assignment1/blob/main/Images/Finite_state_machines.PNG?raw=true "Title""&gt; </p>
<p>Finite state machine diagram </p>
<p>The Normal state is the simplest in nature of the three states, it simply consist of a loop of setting random destinations inside the grid without other interventions while the targets are not achieved.</p>
<p>On the other hand, the sleep consist in setting the target to 'home' (set in the parameter server), and, when the position is achieved, just wait ignoring all signals exept for the change of state.</p>
<p>While the 'Sleep' and 'Normal' state are quite simple in nature, the 'Play' state is quite more complex in nature.</p>
<p>Of course, having to consider the position of the ball without having it, we have to construct a control with the few notions we have: the relative radius of the ball and the angle of the neck with respect to the principal axis of the chassis(that is the principal axis of the robot).</p>
<p>With this, the algorithm is to first minimize the angular offset of the neck w.r.t. the body rotating the body itself and then set a linear velocity while you receive the two data from the Perception node.</p>
<p>Obviously, while this process is happening, the state is checked and change if the change conditions are satisfied.</p>
<p>Although this is the part of the state explicitely coded in the finite state machine, an additional part is present inside the Perception node and is related to how the target is handled and searched.</p>
<p>When the target is achieved, the robot proceeds to what has been defined as "swing routine", in which it moves the neck 45Â° to the left and to the right before centering back again to the target.</p>
<p>When, eventually, the robot loose sight of the ball (always inside the perception code), the robot align the camera to the body and start spinning in the same direction of the last velocity of the body(right if the velocity was to be zero), to try and find it back again, if it cannot manage it, it will switch to the normal state.</p>
<h3>Messages and parameters</h3>
<p>The main parameters used for this implementation are:</p><ul>
<li>ball/ball_description: the gazebo description of the ball</li>
<li>/home x&amp;y: coordinates of the home, location where the pet sleeps</li>
<li>/human_description: gazebo description of the human model</li>
<li>/robot/camera1: set of parameters of the camera sensor</li>
<li>/robot/joint1_position_controller: parameters for the neck joint</li>
<li>/robot/joint_state_controller: parameters for the robot</li>
<li>/robot/robot_description: gazebo robot description</li>
<li>/state: state of the pet(play, normal, sleep)</li>
<li>/gazebo: gazebo anvironment parameters</li>
</ul>
<p>Regarding the messages, they will be listed as</p><ul>
<li>std_msgs.String: used by the commander to the logic, in order to send the command for the ball</li>
<li>std_msgs.Float64: used by the Perception and Behavious to control the neck joint</li>
<li>geometry_msgs:Pose2D: used by Perception to send the radius and camera angle for the control in the play behaviour</li>
<li>geometry_msgs:Twist: used by Behaviours and Following to Gazebo in order to control the twist of the robot</li>
<li>sensor_msgs:JointStates: Used to read the values of the robot states</li>
<li>sensor_msgs:Image: Message with the image camera information</li>
<li>exp_assignment.PlanningAction: message of the action server, it is used by the Pet_logic and Behaviour to use the action server of the ball and robot respectively</li>
</ul>
<h2>Packages and file list</h2>
<p>As already said, the implementation is based on two packages: exp_assignment2 and pet_2. The first handle the simulation of the environment and the movements of the elements in it. In particular, it contains the world, robot and ball description, with the additional control parameters and related topics. The script present in this package are just 2:</p><ul>
<li><a class="el" href="go__to__point__action_8py_source.html">go_to_point_action.py</a>: action server to handle the movement of the robot to a specified target</li>
<li><a class="el" href="go__to__point__ball_8py_source.html">go_to_point_ball.py</a>: action server to move the ball to a specified target Going over to the pet_2 package, this handles the pet from perception to behaviours, plus the ball movement:</li>
<li><a class="el" href="Command__giver_8py.html" title="Component that gives command either randomly generated or given by a user, switching between the two...">Command_giver.py</a>: randomically generate command for the ball to follow</li>
<li>Pet_logic: receives the commands from the command_giver and convert them to movements of the ball, apart from changing the state from play and normal to sleep and sleep to normal.</li>
<li><a class="el" href="Pet__behaviours_8py.html" title="Pet state machine. ">Pet_behaviours.py</a>: is the implementation of the finite state machine, or at least, the entirety of the sleep and normal phase and the control part of the play state</li>
<li><a class="el" href="robot__following_8py.html" title="Perception algorithm which handles the ball tracking and the swing routine. ">robot_following.py</a>: implement the perception part of the robot(in particular the vision), and handles the control of the neck joint, the swing routine and the switch from normal to play and vice versa</li>
</ul>
<p>This were the scripts which are at the core of this implementation, but, at the side, there are other scripts. Starting from exp_assignment2:</p><ul>
<li>gazebo_world.launch: inside the "launch" folder: handles the definition of the simulation environment and of the elements inside it, and the controller for them.</li>
<li>The urdf folder contains the xacro and gazebo description of the ball and robot</li>
<li>the config folder containing the motor_config.yaml, with the controller description</li>
<li>Planning.action is the message of the action server</li>
<li>Finally the world folder contains the world description</li>
</ul>
<h2>Installation and running procedure</h2>
<h3>Installation</h3>
<ul>
<li>Download the package from the github repository</li>
<li>Set the package in the src folder of the catkin workspace</li>
<li>Go to the main folder of the catkin workspace and launch <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;.catkin_make</div></div><!-- fragment --> <h3>Running</h3>
</li>
</ul>
<ul>
<li>After the compiler has completed its task we can use the same shell to launch the simulation environment. <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;roslauch exp_assignment2 gazebo_world.launch</div></div><!-- fragment --></li>
<li>On a differrent shell we can launch the pet control <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;roslauch pet_2 launcher.launch</div></div><!-- fragment --></li>
<li>Now the implementation is up and running</li>
<li>One can observe the world and ball behaviour in the first shell, on the other there shell there is the robot states, and whether the target is achieved</li>
</ul>
<h3>User commands</h3>
<p>While there is the Command_giver to generate random commands, a human user can interface with the implementation, giving command using the following command.</p><ul>
<li>To write a command: <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;rostopic pub /commander std_msgs/String &quot;data: &#39;&#39;&quot; </div></div><!-- fragment --> where, in place of '', you can put any commands as presented before.</li>
<li>Moreover, one can set the state(play(2),normal(1) and sleep(0)), writing: <div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;rosparam /state state_code </div></div><!-- fragment --> state_code is to be substitute with one of the integer code stated</li>
</ul>
<h2>Working assumptions</h2>
<p>The working assumptions will be discussed as the following list:</p><ul>
<li>The robot, simulating a pet, interact with a human with a ball moving in the environment and moves in a 2D surface in a simulation environment.</li>
<li>Both the robot targets and its positions belongs exclusively to the map(16 by 16 grid with center in (0,0))representing the 2D environment.</li>
<li>The robot has 3 main states:<ul>
<li>Play</li>
<li>Normal</li>
<li>Sleep</li>
</ul>
</li>
<li>The logic receive forms in strings with possible form:<ul>
<li>"play"</li>
<li>"hide"</li>
<li>"go to x1 y1" (equivalent to voice command)</li>
<li>"point to x1 y1" (equivalent to pointing commands)</li>
</ul>
</li>
<li>Once logic receives the message it applies it to the ball</li>
<li>if the command is "play", the ball is positioned above ground, if "hide", it is positioned below the ground .</li>
<li>The robot activates the play mode only when the robot perceives the ball.</li>
<li>When the ball is percieved the robot tries to get to it</li>
<li>When achieved the "swing routine" is initialized, for which the neck is first angled to pi/4 and then too -pi/4 and then back to center</li>
<li>If the ball is not in sight anymore, the robot procede to a full rotation to find if the ball is still in the environment, if not, it switch to the normal state..</li>
<li>Two predifined positions inside the map are "Owner" and "Home", which cannot be changed during the execution, and can be used instead of coordinates in giving commands.</li>
</ul>
<h2>System features and limitations</h2>
<p>Starting from the limitations:</p><ul>
<li>The system is not scalable in the number of individually controllable robots, but if all robots have the same state, it is scalable, even if collision between robots are not handled</li>
<li>It is not scalable in the number of symbolic locations</li>
<li>It is not really scalable in the number of states</li>
<li>Does not distinguish between the pointing action and the vocal command, since the ball act</li>
<li>The robot control is not too responsive, since the control can't have a too high gain to avoid instability and the robot toppling over.</li>
<li>The movement speed is high but make a trade-off for instability</li>
<li>There are some problems with the target reaching when it is on the side of the robot especially when really close to the chassis(It cannot handle small curvature radii and the robot tends to run in circles around the target)</li>
<li>Underline jittering in the motors, observable when control is not yet active</li>
</ul>
<p>Going on to the features:</p><ul>
<li>The robot is controlled in almost its entirety by the pet package, which means a high scalability and modularity</li>
<li>We have the robot perspective in a separate window</li>
<li>Can show the location of the robot in the map</li>
<li>The robot can check the state without being stuck in an action server loop</li>
</ul>
<h2>Possible technical improvements</h2>
<p>There are many possible technical improvements to this architecture:</p><ul>
<li>Modify the simulation component to make it more scalable, introducing the state change from and to sleep inside the pet_package</li>
<li>Improve the control, of both the camera and the robot chassis in such a way to perform both linear and angular velocities, and in a way that the robot do not topple over</li>
<li>Handle the situation when the ball is close to the side of the robot, in order to have it centered inside the robot perspective</li>
<li>Add a way to avoid collisions with other obstacles, with the possible introduction of a proximity sensor of some sort</li>
<li>Add multiple robots to the simulation</li>
</ul>
<h2>Author and contacts</h2>
<p>Matteo Palmas: matteo.palmas7gmail.com </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
